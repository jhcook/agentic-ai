# CLI overrides available when running `./agent.py`:
#   --loglevel <LEVEL>       Override LOGLEVEL for current run.
#   --model <MODEL>          Override LLM_NAME (e.g., openai/gpt-4o-mini).
#   --provider <PROVIDER>    Override LLM_PROVIDER (e.g., openai, gemini, ollama).
#   --api-key <KEY>          Override LLM_API_KEY for the session.
#   --api-base <URL>         Override LLM_API_BASE (custom endpoint).
#   --temperature <FLOAT>    Override LLM_TEMPERATURE (0.0–2.0).
#   --who <PERSONA>          Provide persona name without prompting.
#   --question <TEXT>        Provide user question without microphone input.
#   --stream                 Stream responses in real time.
#
# Logging level for the entire application (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOGLEVEL=INFO

# Default model identifier understood by LiteLLM (e.g., openai/gpt-4o, gemini/gemini-1.5-pro, ollama/mistral:7b)
LLM_NAME=ollama/mistral:7b

# Provider name to hint LiteLLM which backend to call (openai, gemini, ollama, anthropic, etc.)
LLM_PROVIDER=ollama

# API key for the chosen provider (use 'ollama' for local Ollama setups, real secret for hosted APIs)
LLM_API_KEY=ollama

# Custom base URL for the provider; set to Ollama’s default, remove or change for hosted APIs
LLM_API_BASE=http://localhost:11434

# Default temperature for completions (0.0 – 2.0). Can be overridden with --temperature at runtime.
LLM_TEMPERATURE=0.0

# Timeout (in seconds) for LiteLLM completion/stream calls
LLM_TIMEOUT=300

# Optional compatibility key for legacy OpenAI integrations or other tools that still expect OPENAI_API_KEY
OPENAI_API_KEY=sk-replace-me
